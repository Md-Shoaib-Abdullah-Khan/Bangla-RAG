# ğŸ“– Bangla RAG Chatbot

**This project implements a Retrieval-Augmented Generation (RAG) based chatbot that can understand and answer both Bangla and English queries. It retrieves context from a PDF knowledge base and uses a Language Model to generate grounded and relevant answers.**  

![Streamlit App](demo.png) 
â–¶ **Watch Demo Video**: [Google Drive Link](link)
---

## ğŸ“‚ **Project Structure**  
```
.
â”œâ”€â”€ app/  
â”‚   â”œâ”€â”€ api.py             # FastAPI backend for RAG queries  
â”‚   â”œâ”€â”€ main.py            # Streamlit chatbot frontend  
â”‚   â”œâ”€â”€ RAG.py             # RAG pipeline implementation  
â”‚   â”œâ”€â”€ evaluation.py      # Evaluates RAG accuracy  
â”‚   â””â”€â”€ test_cases.txt     # Test cases for evaluation
â”œâ”€â”€ data/  
â”‚   â”œâ”€â”€ bangla.pdf         # Input Bangla document  
â”‚   â””â”€â”€ processed.txt      # Preprocessed text  
â”œâ”€â”€ embeddings/            # Stores vector embeddings  
â”œâ”€â”€ evaluation/  
â”‚   â””â”€â”€ test_cases.txt     # Test cases for evaluation  
â”œâ”€â”€ Preprocess.ipynb       # Preprocesses bangla.pdf â†’ processed.txt  
â”œâ”€â”€ requirements.txt       # Python dependencies  
â””â”€â”€ README.md  
```

---

## ğŸ›  **Setup Guide**  

### **1. Install Dependencies**  
```bash
git clone https://github.com/yourusername/bangla-rag-chatbot.git
cd bangla-rag-chatbot
python -m venv myenv
source myenv/bin/activate        # On Windows: myenv\\Scripts\\activate
pip install -r requirements.txt
```

### **2. Configure Environment Variables**  
Create a `.env` file:  
```env
GROQ_API_KEY="your_groq_api_key"
```

### **3. Preprocess Data**  
Run the Jupyter notebook to extract text from `bangla.pdf`:  
```bash
jupyter notebook Preprocess.ipynb
```
*(Output: `data/processed.txt`)*  

### **4. Run the Chatbot**  
**Option 1: FastAPI Backend**  
```bash
python app/api.py
```
â†’ Access API docs at `http://127.0.0.1:8000/docs`  

**Option 2: Streamlit Frontend**  
```bash
streamlit run app/main.py
```
â†’ Opens chatbot at `http://localhost:8501`  

---

## ğŸ§° **Used Tools & Libraries**  
- **LLM**: Groq: `deepseek-r1-distill-llama-70b` & `gemma2-9b-it`
- **Embeddings**: HuggingFace: `l3cube-pune/bengali-sentence-similarity-sbert`  
- **Backend**: FastAPI  
- **Frontend**: Streamlit  
- **Vector DB**: Chroma (local)  
- **Evaluation**: Cosine similarity

*(List all packages in `requirements.txt`)*  

---

## ğŸ“¡ **API Documentation**  
### **POST `/ask`**  
**Input**:  
```json
{"query": "à¦¬à¦¿à§Ÿà§‡à¦° à¦¸à¦®à§Ÿ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦¬à§Ÿà¦¸ à¦•à¦¤ à¦›à¦¿à¦²?"}
```  
**Output**:  
```json
{
  "question": "à¦¬à¦¿à§Ÿà§‡à¦° à¦¸à¦®à§Ÿ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦¬à§Ÿà¦¸ à¦•à¦¤ à¦›à¦¿à¦²?",
  "answer": "à¦ªà¦¨à§‡à¦° à¦¬à¦›à¦°",
}
```

---

## ğŸ” **Sample Query & Output**  
|                  **Query**                      |            **Generated Answer**               |
|-------------------------------------------------|-----------------------------------------------|
| "à¦•à¦¾à¦•à§‡ à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾ à¦¬à¦²à§‡ à¦‰à¦²à§à¦²à§‡à¦– à¦•à¦°à¦¾ à¦¹à§Ÿà§‡à¦›à§‡?" |                  "à¦®à¦¾à¦®à¦¾à¦•à§‡"                     |  

---

## ğŸ“Š **Evaluation Metrics**  
Run evaluation:  
```bash
python app/evaluation.py
```  
-----------------------------------------
Evaluation Results (RAG vs Expected Answers)
-----------------------------------------
1. Query: "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à§Ÿ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à§Ÿà§‡à¦›à§‡?"  
   - Reference Answer: "à¦¶à¦¸à§à¦¤à§à¦¨à¦¾à¦¥ à¦¸à§‡à¦¨à¦•à§‡."  
   - Generated Answer: "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à§Ÿ à¦¸à§à¦ªà§à¦°à§à¦· à¦¶à¦¬à§à¦¦à¦Ÿà¦¿ à¦¶à¦®à§à¦­à§à¦¨à¦¾à¦¥ à¦¸à§‡à¦¨à¦•à§‡ à¦¬à§‹à¦à¦¾à¦¤à§‡ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤ à¦¹à¦¯à¦¼à§‡à¦›à§‡, à¦¯à¦¾à¦° à¦®à¦§à§à¦¯à§‡ à¦ªà§à¦°à§à¦·à¦¾à¦²à¦¿ à¦—à§à¦£à¦¾à¦¬à¦²à¦¿ à¦›à¦¿à¦²à¥¤"  
   - Similarity score: 0.79  

2. Query: "Who is the writer of this story?"  
   - Expected Answer: "The writer of this story is Rabindranath Tagore."  
   - Generated Answer: "The writer of the story "Aparichita" is Rabindranath Tagore."  
   - Similarity score: 0.87 

-----------------------------------------
Average Similarity Score: 0.83  
-----------------------------------------

---

## ğŸ“˜ Evaluation Questions & Answers

### ğŸ“Œ What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?
I used **pdfplumber** and **Tesseract OCR** because the PDF were image-based and contained Bangla script. Yes, I faced challenges such as broken Bangla characters and misaligned MCQs, which were resolved using regex and Unicode normalization.

### ğŸ“Œ What chunking strategy did you choose? Why do you think it works well for semantic retrieval?
We used `RecursiveCharacterTextSplitter` with `chunk_size=200` and `chunk_overlap=50`. It preserves sentence boundaries and maintains context between chunks, which is essential for semantic retrieval.

### ğŸ“Œ What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?
We used **`l3cube-pune/bengali-sentence-similarity-sbert`**, a Bangla-trained Sentence-BERT model. It provides dense semantic embeddings suited for Bangla, making it ideal for question-answer matching.

### ğŸ“Œ How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?
We use **cosine similarity** through **ChromaDB** to match the query embedding against chunk embeddings. Chroma offers fast search and persistence, and cosine similarity is effective for comparing semantic vectors.

### ğŸ“Œ How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?
We:
- Use sentence-level embeddings
- Maintain overlap across chunks
- Feed full retrieved context to the LLM

If the query is vague, the model might give a generic or incorrect response. This can be improved with query rewriting or clarification.

### ğŸ“Œ Do the results seem relevant? If not, what might improve them (e.g. better chunking, better embedding model, larger document)?
Yes, the results are mostly relevant. In edge cases, relevance could be improved by:
- Expanding the corpus
- Using adaptive chunking
- Switching to more powerful or multilingual embedding models

---
## ğŸ“œ License
MIT Â© [Shoaib Khan] - AI Enthusiast Building Multilingual Education Tools

---
